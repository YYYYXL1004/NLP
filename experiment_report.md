# 自然语言处理作业1：基于 spaCy 的中文 NLP 任务评估

**学号**: [你的学号]  
**姓名**: [你的姓名]  
**提交日期**: 2025年11月25日

---

## 1. 实验概述

本次实验旨在利用 **spaCy** 工具库，对**中文分词 (CWS)**、**词性标注 (POS)** 和 **命名实体识别 (NER)** 三项任务进行评估。

虽然作业要求“三选二”，但我对这三个任务都进行了完整的实现和评测。我使用了 spaCy 的中文预训练模型 `zh_core_web_sm`，并在给定的 PKU 格式数据集上进行了测试。

### 1.1 实验环境与工具
- **Python**: 3.8+
- **工具库**: spaCy (v3.x)
- **模型**: `zh_core_web_sm`
- **数据处理**: 针对数据集的 `.txt` 和 `.tsv` 格式编写了专门的加载器。
- **绘图工具**: Matplotlib & Sklearn (用于绘制混淆矩阵)

### 1.2 难点处理：标签映射
实验中最大的挑战在于数据集标签（PKU/人民日报风格）与 spaCy 模型输出标签（OntoNotes/CTB风格）的不一致。
为了准确评估，我查阅了相关文档，构建了一个**映射字典**。
例如，将数据集中的 `nr` (人名) 映射为 spaCy 的 `NR` (名词) 用于POS评估，映射为 `PERSON` 用于NER评估。如果不做这一步，准确率会接近于零。

---

## 2. 任务一：中文分词 (CWS)

### 2.1 评估方法
分词采用**区间匹配法 (Interval Matching)**。只有当一个词的“起始位置”和“结束位置”都与标准答案完全一致时，才算正确。

### 2.2 实验结果

| 指标 | 数值 | 评价 |
| :--- | :--- | :--- |
| **Precision** | 84.76% | 查准率尚可，说明切出来的词大部分是对的。 |
| **Recall** | 85.78% | 查全率略高于查准率。 |
| **F1-Score** | **85.27%** | 整体效果不错，达到了可用水平。 |

![CWS Metrics](images/cws.png)

### 2.3 错误分析 (Bad Case Analysis)
通过程序自动导出的 `bad_cases.txt`，我发现了一些典型错误：
1.  **人名切分问题**：模型倾向于把没见过的人名切开，比如“王小明”切成“王”+“小明”，而标准答案是一个词。
2.  **组合词歧义**：例如“前来”在标准答案里是一个词，但模型切成了“前”+“来”。这种属于分词粒度的定义不同，不完全算模型错误。

---

## 3. 任务二：词性标注 (POS)

### 3.1 评估方法
POS 任务是一个多分类问题，采用 **Accuracy (准确率)** 进行评估。我们对总共数万个词的标签进行了逐一比对。

### 3.2 实验结果

- **Accuracy**: **73.17%**

虽然 73% 看起来不高，但这主要是因为标签体系的映射损失。比如 spaCy 区分了很多助词（DEC, DEG...），而数据集里只有简单的 `u`。尽管做了映射，还是难免有对应不上的情况。

### 3.3 混淆矩阵分析
为了深入分析错误分布，我绘制了 Top-10 常见标签的**混淆矩阵**：

![POS Confusion Matrix](images/pos_confusion.png)

**观察发现**：
- 对角线颜色最深，说明大部分预测是正确的。
- **NN (名词)** 和 **NR (专名)** 之间有较多混淆（图中可以看到颜色较浅的误判块）。这很正常，因为很多人名或地名本身也是名词，界限模糊。
- **VV (动词)** 有时会被误判为 **NN**，这通常发生在“动名兼类词”上，比如“学习”、“经历”。

---

## 4. 任务三：命名实体识别 (NER)

### 4.1 评估方法
NER 任务不仅判断边界，还要判断类型。我统计了 Precision, Recall 和 F1，并专门针对 **人名 (Person)**、**地名 (GPE/Loc)** 和 **机构名 (Org)** 进行了分类统计。

### 4.2 实验结果

| 实体类型 | Precision | Recall | F1-Score | 分析 |
| :--- | :--- | :--- | :--- | :--- |
| **ALL (总分)** | 0.7515 | 0.5651 | **0.6451** | 整体偏低，主要是被机构名拖累了。 |
| **人名 (nr)** | 0.7723 | 0.6985 | **0.7335** | 表现尚可，常见人名都能认出。 |
| **地名 (ns)** | 0.8380 | 0.6747 | **0.7475** | 准确率很高(83%)，但漏识别比较多(Recall低)。 |
| **机构名 (nt)**| 0.4749 | 0.2429 | **0.3214** | **灾难区**。机构名太长太复杂，简称又多，模型很难对齐。 |

![NER Metrics](images/ner.png)

### 4.3 难点总结
NER 是最难的任务。尤其是机构名（Organization），标准答案往往包含完整的“XX委员会”、“XX大学”，而模型经常只识别出其中的“XX”部分，导致 F1 大幅下降。

---

## 5. 扩展工作与亮点

### 5.1 自动化错误报告系统
为了更好地分析错误，我在代码中实现了一个 `save_bad_case` 模块。每次评估时，程序会自动将每个任务的前 5 个典型错误样本（原文、标准、预测）写入 `bad_cases.txt`。这让我能直观地看到模型“傻”在哪里，而不是只盯着冷冰冰的分数。

### 5.2 交互式演示系统 (Interactive Demo)
为了直观体验模型效果，我编写了一个命令行交互功能。跑完测试后，可以直接输入句子进行测试。

**测试案例**：
> 输入：*老师好，我是来自内蒙古大学前来北大交流学习的小姚。*
>
> **分词**: ['老师', '好', '，', '我', '是', '来自', '内蒙古', '大学', '前来', '北大', '交流', '学习', '的', '小姚', '。']  
> **词性**: ['NN', 'VA', 'PU', 'PN', 'VC', 'VV', 'NR', 'NN', 'VV', 'NR', 'VV', 'VV', 'DEC', 'NN', 'PU']  
> **实体**: [('内蒙古大学', 'ORG'), ('北大', 'ORG')]

**结果分析**：
可以看到模型成功识别了“内蒙古大学”和“北大”这两个机构名（ORG），并且词性标注也基本正确（如“小姚”被标为 `NN`，虽然其实是 `NR`，但作为名词也说得过去）。这个 Demo 证明了代码的实用性。

---

## 6. 总结
通过这次实验，我深刻体会到了 NLP 任务中**数据对齐**和**标准统一**的重要性。模型跑分低不一定是模型笨，很可能是因为大家对“什么是词”、“什么是实体”的定义不一样。

虽然过程有点折腾（特别是调映射表和画那个混淆矩阵），但看到最终生成的图表和 Demo 跑通的那一刻，还是挺有成就感的。

（完）
