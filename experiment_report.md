# NLP 基础任务实验报告

## 1. 实验概述

本次实验旨在基于通用 NLP 工具库，在给定的中文数据集上完成中文分词（CWS）、词性标注（POS）和命名实体识别（NER）任务，并评估其性能表现。通过对比模型预测结果与人工标注的标准答案（Gold Standard），分析现有工具在特定语境下的适应性及主要误差来源。

- **实验工具**：spaCy (v3.x)
- **使用模型**：`zh_core_web_sm` (v3.8.0)
- **数据集**：
    - 分词数据：`中文分词.txt` (纯文本，空格分隔)
    - 标注数据：`词性标注.tsv` (Word-Tag pair)
    - 实体数据：`命名实体识别.tsv` (Word-Tag pair)

---

## 2. 任务一：中文分词 (CWS)

### 2.1 实验方法
利用 spaCy 的分词器对原始文本进行切分，将预测结果（Pred）与数据集提供的标准分词（Gold）进行对比。由于分词是序列标注任务，评价采用标准区间匹配法（Interval Matching），即只有当一个词的**起始位置**和**结束位置**都与标准答案一致时，才视为正确。

### 2.2 实验结果

| 指标 | 数值 | 评价 |
| :--- | :--- | :--- |
| **Precision** | 84.76% | 准确率尚可，说明模型切分出的词大部分是合法的词汇。 |
| **Recall** | 85.78% | 召回率与准确率接近，模型表现均衡。 |
| **F1-Score** | **85.27%** | 整体处于可用水平，但距离 SOTA（通常 >95%）仍有差距。 |

### 2.3 错误分析 (Error Analysis)
通过分析 F1 < 0.6 的低分样本，发现主要误差并非来源于模型“切错了”，而是来源于**分词规范（Granularity）的系统性差异**。

#### 典型案例分析
1.  **数词与量词的合并问题**
    - **Gold**: `２万吨`, `４０００万元`, `４４岁`
    - **Pred**: `２万` + `吨`, `４０００万` + `元`, `４４` + `岁`
    - **分析**: 测试集倾向于将“数+量”合并为一个 Token，而 spaCy 的预训练模型倾向于将其拆开。这种切分粒度的差异导致了大量样本的 Precision 和 Recall 同时下降。

2.  **复合专名与组织机构名**
    - **Gold**: `浙江美术学院国画系` (1个词)
    - **Pred**: `浙江` + `美术` + `学院` + `国画` + `系` (5个词)
    - **Gold**: `广东宏远足球队`
    - **Pred**: `广东` + `宏远` + `足球队`
    - **分析**: 数据集将长复合词视为一个最小单元，而模型倾向于按语义成分细分。虽然模型切分在语法上没有错误，但在以“完全匹配”为标准的评估下被判错。

---

## 3. 任务二：词性标注 (POS)

### 3.1 实验方法
词性标注的难点在于**标签体系的映射**。数据集使用的是类 PKU 的标注体系（如 `n`, `nr`, `ns`, `v`, `vn`），而 spaCy 输出的是 OntoNotes/CTB 体系（如 `NOUN`, `VERB`, `PROPN` 以及细粒度的 `NN`, `NR`, `VV`）。

实验中构建了如下映射规则进行对齐：
- `nr` (人名), `ns` (地名), `nt` (机构), `nz` -> `NR` (Proper Noun)
- `n` -> `NN`
- `v`, `vn` -> `VV`
- ... (其他对应项)

### 3.2 实验结果

| 指标 | 数值 | 评价 |
| :--- | :--- | :--- |
| **Accuracy** | **73.17%** | 准确度一般，受制于分词错误的级联影响。 |

### 3.3 结果分析
从生成的混淆矩阵（Confusion Matrix）可以看出：
1.  **级联错误 (Cascading Errors)**：词性标注是建立在分词基础上的。如前所述，`２万吨` 被切开后，模型需要分别标注 `２万` (NUM) 和 `吨` (M/Measure)，而标准答案是 `２万吨` (m/numeral)。这种对齐错位直接导致了标签判错。
2.  **歧义词标注**：在 `v` (动词) 和 `vn` (名动词) 的区分上，不同体系的标准极其模糊，导致大量动词被误判或标签不匹配。

---

## 4. 任务三：命名实体识别 (NER)

### 4.1 实验方法
数据集仅标注了三类实体：`nr` (人名), `ns` (地名/GPE), `nt` (机构名/ORG)。我们将 spaCy 识别出的 `PERSON`, `ORG`, `GPE`, `LOC` 映射回数据集标签进行评估。

### 4.2 实验结果

| 指标 | 数值 | 评价 |
| :--- | :--- | :--- |
| **Precision** | 75.15% | 预测出的实体大部分是正确的。 |
| **Recall** | **56.51%** | **漏报严重**，模型错过了一半左右的实体。 |
| **F1-Score** | 64.51% | 受低召回率拖累，整体分数不高。 |

### 4.3 错误分析
1.  **特定领域实体缺失**：
    - 数据集中包含大量特定语境下的组织名，如 `芙蓉镇常青珠宝有限公司`。通用模型（`zh_core_web_sm` 仅 13MB）对这类长尾实体的覆盖能力有限，常将其识别为普通名词短语而非实体。
2.  **边界界定差异**：
    - 类似于分词问题，模型可能识别出 `宏远` 是 `ORG`，但数据集标注的是 `广东宏远足球队` 整体。这种边界不一致直接导致 Recall 大幅降低。

---

## 5. 总结

本次实验结果表明，直接将通用领域的预训练模型（Off-the-shelf Model）应用于特定数据集时，**“标准不一致”是比“模型能力”更严重的问题**。

1.  **分词粒度**是影响 F1 的首要因素。若需在实际工程中提升分数，首要措施不是更换模型，而是引入**自定义词典**或**规则后处理**，强制合并“数+量”结构和复合专名。
2.  **NER 的低召回率**提示我们，小型的通用模型在处理特定业务数据时往往力不从心。解决路径通常需要针对垂直领域数据进行微调（Fine-tuning）。

（图表详情请参阅 `images/` 目录下生成的 `cws_metrics.png`, `pos_confusion_matrix.png` 及 `ner_metrics.png`）
